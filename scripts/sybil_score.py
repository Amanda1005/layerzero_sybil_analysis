import pandas as pd
import numpy as np
import json
import logging
from datetime import datetime
import os

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class SybilScorer:
    def __init__(self):
        """å¥³å·«é¢¨éšªè©•åˆ†ç³»çµ±"""
        self.transactions_df = None
        self.cluster_data = None
        self.address_scores = {}
        
        # è©•åˆ†æ¬Šé‡é…ç½®
        self.scoring_weights = {
            'pathway_similarity': 40,      # è·¯å¾‘ç›¸ä¼¼æ€§æ¬Šé‡æœ€é«˜
            'temporal_coordination': 25,   # æ™‚é–“å”èª¿æ€§
            'behavioral_similarity': 20,   # è¡Œç‚ºç›¸ä¼¼æ€§
            'network_density': 15          # ç¶²çµ¡å¯†åº¦
        }
    
    def load_data(self):
        """è¼‰å…¥æ‰€æœ‰åˆ†ææ•¸æ“š"""
        try:
            # è¼‰å…¥äº¤æ˜“æ•¸æ“š
            self.transactions_df = pd.read_csv("data/layerzero_transactions.csv")
            # ç¢ºä¿æ™‚é–“æ¬„ä½æ ¼å¼æ­£ç¢º
            self.transactions_df['block_timestamp'] = pd.to_datetime(self.transactions_df['block_timestamp'])
            logging.info(f"ğŸ“– è¼‰å…¥äº¤æ˜“æ•¸æ“š: {len(self.transactions_df)} ç­†")
            
            # è¼‰å…¥é›†ç¾¤åˆ†æçµæœ
            with open("data/cluster_analysis_report.json", "r", encoding='utf-8') as f:
                self.cluster_data = json.load(f)
            logging.info("ğŸ“– è¼‰å…¥é›†ç¾¤åˆ†æçµæœ")
            
            return True
        except FileNotFoundError as e:
            logging.error(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {str(e)}")
            return False
    
    def calculate_pathway_similarity_score(self, address):
        """è¨ˆç®—è·¯å¾‘ç›¸ä¼¼æ€§åˆ†æ•¸ (0-100)"""
        # æª¢æŸ¥æ˜¯å¦åœ¨è·¯å¾‘é›†ç¾¤ä¸­
        pathway_clusters = self.cluster_data.get('clusters', {}).get('pathway', {}).get('details', [])
        
        for cluster in pathway_clusters:
            if address in cluster.get('addresses', []):
                cluster_size = cluster.get('size', 0)
                # é›†ç¾¤è¶Šå¤§ï¼Œé¢¨éšªè¶Šé«˜
                if cluster_size >= 20:
                    return 100  # æœ€é«˜é¢¨éšª
                elif cluster_size >= 10:
                    return 85
                elif cluster_size >= 5:
                    return 70
                else:
                    return 50
        
        return 10  # ä¸åœ¨ä»»ä½•è·¯å¾‘é›†ç¾¤ä¸­
    
    def calculate_temporal_coordination_score(self, address):
        """è¨ˆç®—æ™‚é–“å”èª¿æ€§åˆ†æ•¸ (0-100)"""
        temporal_clusters = self.cluster_data.get('clusters', {}).get('temporal', {}).get('details', [])
        
        max_score = 0
        for cluster in temporal_clusters:
            if address in cluster.get('addresses', []):
                unique_addresses = cluster.get('unique_addresses', 1)
                transaction_count = cluster.get('transaction_count', 1)
                
                # è¨ˆç®—é›†ä¸­åº¦åˆ†æ•¸
                if unique_addresses >= 15 and transaction_count >= 100:
                    score = 100
                elif unique_addresses >= 10 and transaction_count >= 50:
                    score = 85
                elif unique_addresses >= 5 and transaction_count >= 20:
                    score = 70
                else:
                    score = 40
                
                max_score = max(max_score, score)
        
        return max_score
    
    def calculate_behavioral_similarity_score(self, address):
        """è¨ˆç®—è¡Œç‚ºç›¸ä¼¼æ€§åˆ†æ•¸ (0-100)"""
        similarity_clusters = self.cluster_data.get('clusters', {}).get('similarity', {}).get('details', [])
        
        for cluster in similarity_clusters:
            if address in cluster.get('addresses', []):
                cluster_size = cluster.get('size', 0)
                # è¡Œç‚ºç›¸ä¼¼é›†ç¾¤å¤§å°è©•åˆ†
                if cluster_size >= 10:
                    return 90
                elif cluster_size >= 5:
                    return 75
                elif cluster_size >= 3:
                    return 60
                else:
                    return 40
        
        return 20  # ä¸åœ¨ç›¸ä¼¼æ€§é›†ç¾¤ä¸­
    
    def calculate_network_density_score(self, address):
        """è¨ˆç®—ç¶²çµ¡å¯†åº¦åˆ†æ•¸ (0-100)"""
        # å¾äº¤æ˜“æ•¸æ“šè¨ˆç®—è©²åœ°å€çš„ç¶²çµ¡ç‰¹å¾µ
        addr_txs = self.transactions_df[self.transactions_df['address'] == address]
        
        if len(addr_txs) == 0:
            return 0
        
        # è¨ˆç®—ç‰¹å¾µ
        tx_count = len(addr_txs)
        unique_pathways = addr_txs['pathway_id'].nunique()
        time_span = (addr_txs['block_timestamp'].max() - addr_txs['block_timestamp'].min())
        
        # è©•åˆ†é‚è¼¯
        score = 0
        
        # äº¤æ˜“æ•¸é‡ä¸€è‡´æ€§ï¼ˆå¥³å·«è¾²å ´é€šå¸¸æœ‰ç›¸åŒçš„äº¤æ˜“æ•¸ï¼‰
        if tx_count == 8:  # æ ¹æ“šè§€å¯Ÿåˆ°çš„æ¨¡å¼
            score += 30
        elif 6 <= tx_count <= 10:
            score += 20
        else:
            score += 5
        
        # è·¯å¾‘å¤šæ¨£æ€§ï¼ˆä½å¤šæ¨£æ€§ = é«˜é¢¨éšªï¼‰
        if unique_pathways <= 2:
            score += 25
        elif unique_pathways <= 4:
            score += 15
        else:
            score += 5
        
        # æ™‚é–“é›†ä¸­æ€§
        if pd.notna(time_span) and hasattr(time_span, 'total_seconds'):
            time_hours = time_span.total_seconds() / 3600
            if time_hours <= 2:
                score += 30
            elif time_hours <= 6:
                score += 20
            else:
                score += 10
        else:
            score += 5
        
        return min(score, 100)
    
    def calculate_composite_score(self, address):
        """è¨ˆç®—ç¶œåˆå¥³å·«é¢¨éšªåˆ†æ•¸"""
        scores = {
            'pathway_similarity': self.calculate_pathway_similarity_score(address),
            'temporal_coordination': self.calculate_temporal_coordination_score(address),
            'behavioral_similarity': self.calculate_behavioral_similarity_score(address),
            'network_density': self.calculate_network_density_score(address)
        }
        
        # åŠ æ¬Šè¨ˆç®—ç¸½åˆ†
        weighted_score = sum(
            scores[component] * (self.scoring_weights[component] / 100)
            for component in scores
        )
        
        return {
            'address': address,
            'composite_score': round(weighted_score, 2),
            'risk_level': self.get_risk_level(weighted_score),
            'component_scores': scores,
            'confidence': self.calculate_confidence(scores)
        }
    
    def get_risk_level(self, score):
        """æ ¹æ“šåˆ†æ•¸ç¢ºå®šé¢¨éšªç­‰ç´š"""
        if score >= 90:
            return "CRITICAL - æ¥µé«˜å¥³å·«é¢¨éšª"
        elif score >= 75:
            return "HIGH - é«˜å¥³å·«é¢¨éšª"
        elif score >= 60:
            return "MEDIUM - ä¸­ç­‰å¥³å·«é¢¨éšª"
        elif score >= 40:
            return "LOW - ä½å¥³å·«é¢¨éšª"
        else:
            return "MINIMAL - æœ€å°é¢¨éšª"
    
    def calculate_confidence(self, scores):
        """è¨ˆç®—é æ¸¬ä¿¡å¿ƒåº¦"""
        # å¦‚æœå¤šå€‹æŒ‡æ¨™éƒ½å¾ˆé«˜ï¼Œä¿¡å¿ƒåº¦å°±é«˜
        high_scores = sum(1 for score in scores.values() if score >= 80)
        medium_scores = sum(1 for score in scores.values() if 60 <= score < 80)
        
        if high_scores >= 3:
            return "VERY_HIGH"
        elif high_scores >= 2:
            return "HIGH"
        elif high_scores >= 1 or medium_scores >= 3:
            return "MEDIUM"
        else:
            return "LOW"
    
    def score_all_addresses(self):
        """ç‚ºæ‰€æœ‰åœ°å€è©•åˆ†"""
        logging.info("ğŸ¯ é–‹å§‹è¨ˆç®—æ‰€æœ‰åœ°å€çš„å¥³å·«é¢¨éšªåˆ†æ•¸...")
        
        addresses = self.transactions_df['address'].unique()
        scored_addresses = []
        
        for address in addresses:
            score_result = self.calculate_composite_score(address)
            scored_addresses.append(score_result)
            logging.info(f"âœ… {address}: {score_result['composite_score']:.1f} ({score_result['risk_level']})")
        
        # æŒ‰åˆ†æ•¸æ’åº
        scored_addresses.sort(key=lambda x: x['composite_score'], reverse=True)
        
        return scored_addresses
    
    def generate_final_report(self, scored_addresses):
        """ç”Ÿæˆæœ€çµ‚å¥³å·«åˆ†æå ±å‘Š"""
        logging.info("ğŸ“‹ ç”Ÿæˆæœ€çµ‚å¥³å·«åˆ†æå ±å‘Š...")
        
        # çµ±è¨ˆåˆ†æ
        scores = [addr['composite_score'] for addr in scored_addresses]
        risk_levels = [addr['risk_level'] for addr in scored_addresses]
        
        report = {
            'analysis_metadata': {
                'timestamp': datetime.now().isoformat(),
                'total_addresses': len(scored_addresses),
                'total_transactions': len(self.transactions_df),
                'analysis_version': '1.0'
            },
            'summary_statistics': {
                'average_score': np.mean(scores),
                'median_score': np.median(scores),
                'max_score': max(scores),
                'min_score': min(scores),
                'std_score': np.std(scores)
            },
            'risk_distribution': {
                level: len([r for r in risk_levels if level in r])
                for level in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'MINIMAL']
            },
            'detailed_scores': scored_addresses,
            'recommendations': self.generate_recommendations(scored_addresses)
        }
        
        # å„²å­˜ JSON å ±å‘Š
        with open("data/final_sybil_report.json", "w", encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        # å„²å­˜ CSV æ‘˜è¦
        summary_data = []
        for addr_data in scored_addresses:
            summary_data.append({
                'address': addr_data['address'],
                'composite_score': addr_data['composite_score'],
                'risk_level': addr_data['risk_level'],
                'confidence': addr_data['confidence'],
                'pathway_score': addr_data['component_scores']['pathway_similarity'],
                'temporal_score': addr_data['component_scores']['temporal_coordination'],
                'behavioral_score': addr_data['component_scores']['behavioral_similarity'],
                'network_score': addr_data['component_scores']['network_density']
            })
        
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_csv("data/final_sybil_scores.csv", index=False)
        
        logging.info("ğŸ’¾ æœ€çµ‚å ±å‘Šå·²å„²å­˜:")
        logging.info("  - data/final_sybil_report.json")
        logging.info("  - data/final_sybil_scores.csv")
        
        return report
    
    def generate_recommendations(self, scored_addresses):
        """ç”Ÿæˆå»ºè­°"""
        critical_addresses = [addr for addr in scored_addresses if addr['composite_score'] >= 90]
        high_risk_addresses = [addr for addr in scored_addresses if 75 <= addr['composite_score'] < 90]
        
        recommendations = [
            f"ç™¼ç¾ {len(critical_addresses)} å€‹æ¥µé«˜é¢¨éšªå¥³å·«åœ°å€ï¼Œå»ºè­°ç«‹å³åˆ—å…¥é»‘åå–®",
            f"ç™¼ç¾ {len(high_risk_addresses)} å€‹é«˜é¢¨éšªåœ°å€ï¼Œå»ºè­°é€²ä¸€æ­¥èª¿æŸ¥",
            "æ‰€æœ‰åœ°å€éƒ½é¡¯ç¤ºå‡ºæ¥µåº¦ç›¸ä¼¼çš„è¡Œç‚ºæ¨¡å¼ï¼Œå¼·çƒˆå»ºè­°é€²è¡Œå…¨é¢å¯©æŸ¥",
            "å»ºè­°å»ºç«‹è‡ªå‹•ç›£æ§ç³»çµ±ï¼Œæª¢æ¸¬é¡ä¼¼çš„å”èª¿è¡Œç‚ºæ¨¡å¼"
        ]
        
        return recommendations
    
    def print_summary_report(self, report):
        """æ‰“å°æ‘˜è¦å ±å‘Š"""
        print("\n" + "="*70)
        print("ğŸ•µï¸ LayerZero å¥³å·«åœ°å€åˆ†æ - æœ€çµ‚å ±å‘Š")
        print("="*70)
        
        meta = report['analysis_metadata']
        stats = report['summary_statistics']
        distribution = report['risk_distribution']
        
        print(f"ğŸ“Š åˆ†ææ¦‚è¦½:")
        print(f"  ç¸½åœ°å€æ•¸: {meta['total_addresses']}")
        print(f"  ç¸½äº¤æ˜“æ•¸: {meta['total_transactions']}")
        print(f"  å¹³å‡é¢¨éšªåˆ†æ•¸: {stats['average_score']:.1f}/100")
        print(f"  æœ€é«˜é¢¨éšªåˆ†æ•¸: {stats['max_score']:.1f}/100")
        
        print(f"\nğŸš¨ é¢¨éšªç­‰ç´šåˆ†ä½ˆ:")
        for level, count in distribution.items():
            if count > 0:
                percentage = (count / meta['total_addresses']) * 100
                print(f"  {level}: {count} å€‹åœ°å€ ({percentage:.1f}%)")
        
        print(f"\nğŸ¯ TOP 5 æœ€é«˜é¢¨éšªåœ°å€:")
        for i, addr in enumerate(report['detailed_scores'][:5], 1):
            print(f"  {i}. {addr['address']}")
            print(f"     é¢¨éšªåˆ†æ•¸: {addr['composite_score']:.1f}/100")
            print(f"     é¢¨éšªç­‰ç´š: {addr['risk_level']}")
            print(f"     ä¿¡å¿ƒåº¦: {addr['confidence']}")
        
        print(f"\nğŸ’¡ å»ºè­°:")
        for i, rec in enumerate(report['recommendations'], 1):
            print(f"  {i}. {rec}")
        
        print("="*70)

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    
    # åˆå§‹åŒ–è©•åˆ†å™¨
    scorer = SybilScorer()
    
    # è¼‰å…¥æ•¸æ“š
    if not scorer.load_data():
        return
    
    # è¨ˆç®—æ‰€æœ‰åœ°å€åˆ†æ•¸
    scored_addresses = scorer.score_all_addresses()
    
    # ç”Ÿæˆæœ€çµ‚å ±å‘Š
    final_report = scorer.generate_final_report(scored_addresses)
    
    # æ‰“å°æ‘˜è¦
    scorer.print_summary_report(final_report)
    
    logging.info("ğŸ¯ å¥³å·«é¢¨éšªè©•åˆ†å®Œæˆï¼")

if __name__ == "__main__":
    main()