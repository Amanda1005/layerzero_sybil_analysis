import pandas as pd
import numpy as np
import networkx as nx
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity
import logging
from datetime import datetime, timedelta
import json
import os
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class SybilClusterAnalyzer:
    def __init__(self):
        """å¥³å·«é›†ç¾¤åˆ†æå™¨"""
        self.transactions_df = None
        self.address_features = None
        self.similarity_graph = None
        self.clusters = {}
        
    def load_data(self, tx_file="data/layerzero_transactions.csv"):
        """è¼‰å…¥äº¤æ˜“è³‡æ–™"""
        try:
            self.transactions_df = pd.read_csv(tx_file)
            logging.info(f"ğŸ“– è¼‰å…¥ {len(self.transactions_df)} ç­†äº¤æ˜“ï¼Œ{self.transactions_df['address'].nunique()} å€‹åœ°å€")
            
            # è³‡æ–™é è™•ç†
            self.transactions_df['block_timestamp'] = pd.to_datetime(self.transactions_df['block_timestamp'])
            self.transactions_df['created'] = pd.to_datetime(self.transactions_df['created'])
            
            return True
        except FileNotFoundError:
            logging.error(f"âŒ æ‰¾ä¸åˆ°äº¤æ˜“æª”æ¡ˆ: {tx_file}")
            return False
    
    def extract_behavioral_features(self):
        """æå–åœ°å€è¡Œç‚ºç‰¹å¾µ"""
        logging.info("ğŸ” æå–åœ°å€è¡Œç‚ºç‰¹å¾µ...")
        
        features_list = []
        
        for address in self.transactions_df['address'].unique():
            addr_txs = self.transactions_df[self.transactions_df['address'] == address]
            
            # åŸºæœ¬çµ±è¨ˆç‰¹å¾µ
            basic_features = {
                'address': address,
                'tx_count': len(addr_txs),
                'unique_src_chains': addr_txs['src_eid'].nunique(),
                'unique_dst_chains': addr_txs['dst_eid'].nunique(),
                'unique_pathways': addr_txs['pathway_id'].nunique(),
            }
            
            # æ™‚é–“ç‰¹å¾µ
            timestamps = addr_txs['block_timestamp'].dropna()
            if len(timestamps) > 1:
                time_diffs = timestamps.diff().dropna()
                time_features = {
                    'first_tx_time': timestamps.min(),
                    'last_tx_time': timestamps.max(),
                    'tx_timespan_hours': (timestamps.max() - timestamps.min()).total_seconds() / 3600,
                    'avg_interval_minutes': time_diffs.dt.total_seconds().mean() / 60,
                    'std_interval_minutes': time_diffs.dt.total_seconds().std() / 60,
                    'min_interval_seconds': time_diffs.dt.total_seconds().min(),
                    'max_interval_seconds': time_diffs.dt.total_seconds().max(),
                }
            else:
                time_features = {
                    'first_tx_time': timestamps.min() if len(timestamps) > 0 else None,
                    'last_tx_time': timestamps.max() if len(timestamps) > 0 else None,
                    'tx_timespan_hours': 0,
                    'avg_interval_minutes': 0,
                    'std_interval_minutes': 0,
                    'min_interval_seconds': 0,
                    'max_interval_seconds': 0,
                }
            
            # è·¯å¾‘æ¨¡å¼ç‰¹å¾µ
            pathway_patterns = {
                'most_common_src_eid': addr_txs['src_eid'].mode().iloc[0] if len(addr_txs['src_eid'].mode()) > 0 else None,
                'most_common_dst_eid': addr_txs['dst_eid'].mode().iloc[0] if len(addr_txs['dst_eid'].mode()) > 0 else None,
                'src_eid_entropy': self._calculate_entropy(addr_txs['src_eid']),
                'dst_eid_entropy': self._calculate_entropy(addr_txs['dst_eid']),
            }
            
            # nonce æ¨¡å¼
            nonces = addr_txs['nonce'].dropna()
            nonce_features = {
                'nonce_range': nonces.max() - nonces.min() if len(nonces) > 0 else 0,
                'nonce_gaps': len(nonces) - (nonces.max() - nonces.min() + 1) if len(nonces) > 0 else 0,
                'sequential_nonces': (nonces.diff().dropna() == 1).sum() if len(nonces) > 1 else 0,
            }
            
            # åˆä½µæ‰€æœ‰ç‰¹å¾µ
            features = {**basic_features, **time_features, **pathway_patterns, **nonce_features}
            features_list.append(features)
        
        self.address_features = pd.DataFrame(features_list)
        logging.info(f"âœ… æå–å®Œæˆï¼Œå…± {len(self.address_features)} å€‹åœ°å€çš„ {len(self.address_features.columns)-1} å€‹ç‰¹å¾µ")
        
        return self.address_features
    
    def _calculate_entropy(self, series):
        """è¨ˆç®—è³‡è¨Šç†µ"""
        value_counts = series.value_counts()
        probabilities = value_counts / len(series)
        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))
        return entropy
    
    def calculate_address_similarity(self):
        """è¨ˆç®—åœ°å€é–“çš„ç›¸ä¼¼åº¦"""
        logging.info("ğŸ”— è¨ˆç®—åœ°å€é–“ç›¸ä¼¼åº¦...")
        
        if self.address_features is None:
            logging.error("âŒ è«‹å…ˆæå–ç‰¹å¾µ")
            return None
        
        # é¸æ“‡æ•¸å€¼ç‰¹å¾µ
        numeric_features = self.address_features.select_dtypes(include=[np.number]).drop(columns=['address'], errors='ignore')
        
        # æ¨™æº–åŒ–ç‰¹å¾µ
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(numeric_features.fillna(0))
        
        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        similarity_matrix = cosine_similarity(features_scaled)
        
        # å»ºç«‹ç›¸ä¼¼åº¦åœ–
        self.similarity_graph = nx.Graph()
        addresses = self.address_features['address'].tolist()
        
        # æ·»åŠ ç¯€é»
        for i, addr in enumerate(addresses):
            self.similarity_graph.add_node(addr, features=features_scaled[i])
        
        # æ·»åŠ é‚Šï¼ˆç›¸ä¼¼åº¦é–¾å€¼ï¼‰
        similarity_threshold = 0.95  # é«˜ç›¸ä¼¼åº¦é–¾å€¼
        for i in range(len(addresses)):
            for j in range(i+1, len(addresses)):
                similarity = similarity_matrix[i, j]
                if similarity >= similarity_threshold:
                    self.similarity_graph.add_edge(
                        addresses[i], 
                        addresses[j], 
                        weight=similarity,
                        similarity=similarity
                    )
        
        logging.info(f"âœ… ç›¸ä¼¼åº¦åœ–å»ºç«‹å®Œæˆï¼š{len(self.similarity_graph.nodes)} å€‹ç¯€é»ï¼Œ{len(self.similarity_graph.edges)} æ¢é‚Š")
        return similarity_matrix
    
    def detect_temporal_clusters(self, time_window_minutes=30):
        """æª¢æ¸¬æ™‚é–“é›†ç¾¤"""
        logging.info(f"â° æª¢æ¸¬æ™‚é–“é›†ç¾¤ï¼ˆçª—å£ï¼š{time_window_minutes} åˆ†é˜ï¼‰...")
        
        temporal_clusters = []
        
        # æŒ‰æ™‚é–“æ’åºæ‰€æœ‰äº¤æ˜“
        all_txs = self.transactions_df.sort_values('block_timestamp')
        
        current_cluster = []
        cluster_start_time = None
        
        for _, tx in all_txs.iterrows():
            tx_time = tx['block_timestamp']
            
            if cluster_start_time is None:
                # é–‹å§‹æ–°é›†ç¾¤
                cluster_start_time = tx_time
                current_cluster = [tx]
            elif (tx_time - cluster_start_time).total_seconds() <= time_window_minutes * 60:
                # åœ¨æ™‚é–“çª—å£å…§
                current_cluster.append(tx)
            else:
                # è¶…å‡ºæ™‚é–“çª—å£ï¼Œä¿å­˜ç•¶å‰é›†ç¾¤ä¸¦é–‹å§‹æ–°é›†ç¾¤
                if len(current_cluster) >= 2:  # è‡³å°‘2ç­†äº¤æ˜“æ‰ç®—é›†ç¾¤
                    temporal_clusters.append({
                        'cluster_id': len(temporal_clusters),
                        'start_time': cluster_start_time,
                        'end_time': current_cluster[-1]['block_timestamp'],
                        'addresses': list(set([tx['address'] for tx in current_cluster])),
                        'transaction_count': len(current_cluster),
                        'unique_addresses': len(set([tx['address'] for tx in current_cluster]))
                    })
                
                cluster_start_time = tx_time
                current_cluster = [tx]
        
        # è™•ç†æœ€å¾Œä¸€å€‹é›†ç¾¤
        if len(current_cluster) >= 2:
            temporal_clusters.append({
                'cluster_id': len(temporal_clusters),
                'start_time': cluster_start_time,
                'end_time': current_cluster[-1]['block_timestamp'],
                'addresses': list(set([tx['address'] for tx in current_cluster])),
                'transaction_count': len(current_cluster),
                'unique_addresses': len(set([tx['address'] for tx in current_cluster]))
            })
        
        self.clusters['temporal'] = temporal_clusters
        logging.info(f"âœ… ç™¼ç¾ {len(temporal_clusters)} å€‹æ™‚é–“é›†ç¾¤")
        
        return temporal_clusters
    
    def detect_pathway_clusters(self):
        """æª¢æ¸¬ç›¸åŒè·¯å¾‘é›†ç¾¤"""
        logging.info("ğŸ›¤ï¸ æª¢æ¸¬è·¯å¾‘æ¨¡å¼é›†ç¾¤...")
        
        # åˆ†æè·¯å¾‘åºåˆ—æ¨¡å¼
        pathway_clusters = defaultdict(list)
        
        for address in self.transactions_df['address'].unique():
            addr_txs = self.transactions_df[self.transactions_df['address'] == address].sort_values('block_timestamp')
            
            # å»ºç«‹è·¯å¾‘åºåˆ—
            pathway_sequence = []
            for _, tx in addr_txs.iterrows():
                pathway_sequence.append(f"{tx['src_eid']}->{tx['dst_eid']}")
            
            # ä½¿ç”¨è·¯å¾‘åºåˆ—ä½œç‚ºé›†ç¾¤key
            pathway_pattern = "|".join(pathway_sequence)
            pathway_clusters[pathway_pattern].append(address)
        
        # è½‰æ›ç‚ºåˆ—è¡¨æ ¼å¼
        pathway_cluster_list = []
        for pattern, addresses in pathway_clusters.items():
            if len(addresses) >= 2:  # è‡³å°‘2å€‹åœ°å€æ‰ç®—é›†ç¾¤
                pathway_cluster_list.append({
                    'cluster_id': len(pathway_cluster_list),
                    'pattern': pattern,
                    'addresses': addresses,
                    'size': len(addresses)
                })
        
        self.clusters['pathway'] = pathway_cluster_list
        logging.info(f"âœ… ç™¼ç¾ {len(pathway_cluster_list)} å€‹è·¯å¾‘æ¨¡å¼é›†ç¾¤")
        
        return pathway_cluster_list
    
    def detect_similarity_clusters(self, eps=0.1, min_samples=2):
        """ä½¿ç”¨ DBSCAN æª¢æ¸¬ç›¸ä¼¼åº¦é›†ç¾¤"""
        logging.info("ğŸ¯ ä½¿ç”¨ DBSCAN æª¢æ¸¬è¡Œç‚ºç›¸ä¼¼é›†ç¾¤...")
        
        if self.address_features is None:
            logging.error("âŒ è«‹å…ˆæå–ç‰¹å¾µ")
            return []
        
        # æº–å‚™æ•¸å€¼ç‰¹å¾µ
        numeric_features = self.address_features.select_dtypes(include=[np.number])
        features_scaled = StandardScaler().fit_transform(numeric_features.fillna(0))
        
        # DBSCAN èšé¡
        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')
        cluster_labels = dbscan.fit_predict(features_scaled)
        
        # æ•´ç†èšé¡çµæœ
        similarity_clusters = []
        for cluster_id in set(cluster_labels):
            if cluster_id != -1:  # æ’é™¤å™ªéŸ³é»
                cluster_addresses = self.address_features[cluster_labels == cluster_id]['address'].tolist()
                similarity_clusters.append({
                    'cluster_id': cluster_id,
                    'addresses': cluster_addresses,
                    'size': len(cluster_addresses)
                })
        
        self.clusters['similarity'] = similarity_clusters
        logging.info(f"âœ… ç™¼ç¾ {len(similarity_clusters)} å€‹è¡Œç‚ºç›¸ä¼¼é›†ç¾¤")
        
        return similarity_clusters
    
    def generate_cluster_report(self):
        """ç”Ÿæˆé›†ç¾¤åˆ†æå ±å‘Š"""
        logging.info("ğŸ“‹ ç”Ÿæˆé›†ç¾¤åˆ†æå ±å‘Š...")
        
        report = {
            'analysis_timestamp': datetime.now().isoformat(),
            'total_addresses': self.address_features['address'].nunique() if self.address_features is not None else 0,
            'total_transactions': len(self.transactions_df) if self.transactions_df is not None else 0,
            'clusters': {}
        }
        
        for cluster_type, clusters in self.clusters.items():
            report['clusters'][cluster_type] = {
                'count': len(clusters),
                'details': clusters
            }
        
        # å„²å­˜å ±å‘Š
        os.makedirs("data", exist_ok=True)
        with open("data/cluster_analysis_report.json", "w", encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¹Ÿå„²å­˜ç‚º CSV æ ¼å¼çš„æ‘˜è¦
        summary_data = []
        for cluster_type, clusters in self.clusters.items():
            for cluster in clusters:
                summary_data.append({
                    'cluster_type': cluster_type,
                    'cluster_id': cluster['cluster_id'],
                    'size': cluster.get('size', len(cluster.get('addresses', []))),
                    'addresses': ','.join(cluster.get('addresses', [])),
                    'pattern': cluster.get('pattern', ''),
                    'risk_score': len(cluster.get('addresses', [])) * 10  # åˆæ­¥é¢¨éšªåˆ†æ•¸
                })
        
        summary_df = pd.DataFrame(summary_data)
        if not summary_df.empty:
            summary_df.to_csv("data/cluster_summary.csv", index=False)
            logging.info("ğŸ’¾ é›†ç¾¤æ‘˜è¦å·²å„²å­˜è‡³ data/cluster_summary.csv")
        
        return report
    
    def print_analysis_summary(self):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ•µï¸ å¥³å·«é›†ç¾¤åˆ†æå ±å‘Š")
        print("="*60)
        
        if self.address_features is not None:
            print(f"ğŸ“Š ç¸½åœ°å€æ•¸: {self.address_features['address'].nunique()}")
            print(f"ğŸ“ˆ ç¸½äº¤æ˜“æ•¸: {len(self.transactions_df)}")
        
        for cluster_type, clusters in self.clusters.items():
            print(f"\nğŸ” {cluster_type.upper()} é›†ç¾¤:")
            if clusters:
                for cluster in clusters[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                    size = cluster.get('size', len(cluster.get('addresses', [])))
                    print(f"  é›†ç¾¤ {cluster['cluster_id']}: {size} å€‹åœ°å€")
                    if 'pattern' in cluster:
                        print(f"    æ¨¡å¼: {cluster['pattern'][:100]}...")
                
                if len(clusters) > 5:
                    print(f"  ... é‚„æœ‰ {len(clusters)-5} å€‹é›†ç¾¤")
            else:
                print("  æœªç™¼ç¾é›†ç¾¤")
        
        print("="*60)

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = SybilClusterAnalyzer()
    
    # è¼‰å…¥è³‡æ–™
    if not analyzer.load_data():
        return
    
    # æå–ç‰¹å¾µ
    features = analyzer.extract_behavioral_features()
    
    # è¨ˆç®—ç›¸ä¼¼åº¦
    similarity_matrix = analyzer.calculate_address_similarity()
    
    # æª¢æ¸¬å„ç¨®é¡å‹çš„é›†ç¾¤
    temporal_clusters = analyzer.detect_temporal_clusters(time_window_minutes=30)
    pathway_clusters = analyzer.detect_pathway_clusters()
    similarity_clusters = analyzer.detect_similarity_clusters(eps=0.1, min_samples=2)
    
    # ç”Ÿæˆå ±å‘Š
    report = analyzer.generate_cluster_report()
    
    # æ‰“å°æ‘˜è¦
    analyzer.print_analysis_summary()
    
    logging.info("ğŸ¯ é›†ç¾¤åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()