#!/usr/bin/env python3
"""
æª¢æŸ¥é …ç›®ç‹€æ…‹å’Œæ•¸æ“šæ–‡ä»¶
"""

import os
import pandas as pd
import json
from pathlib import Path

def check_project_status():
    """æª¢æŸ¥é …ç›®ç•¶å‰ç‹€æ…‹"""
    print("ğŸ” LayerZero å¥³å·«åˆ†æé …ç›®ç‹€æ…‹æª¢æŸ¥")
    print("=" * 50)
    
    # æª¢æŸ¥ç›®éŒ„çµæ§‹
    print("\nğŸ“ ç›®éŒ„çµæ§‹:")
    base_dirs = ['data', 'scripts', 'notebooks', 'dashboard']
    for dir_name in base_dirs:
        if os.path.exists(dir_name):
            print(f"âœ… {dir_name}/")
            # åˆ—å‡ºç›®éŒ„å…§å®¹
            for file in os.listdir(dir_name):
                file_path = os.path.join(dir_name, file)
                size = os.path.getsize(file_path) if os.path.isfile(file_path) else 0
                print(f"   ğŸ“„ {file} ({size} bytes)")
        else:
            print(f"âŒ {dir_name}/ (ä¸å­˜åœ¨)")
    
    # æª¢æŸ¥é—œéµæ•¸æ“šæ–‡ä»¶
    print("\nğŸ“Š é—œéµæ•¸æ“šæ–‡ä»¶:")
    required_files = {
        'data/sybil_addresses.csv': 'åŸå§‹å¥³å·«åœ°å€åˆ—è¡¨',
        'data/layerzero_transactions.csv': 'äº¤æ˜“æ•¸æ“š',
        'data/cluster_analysis_report.json': 'é›†ç¾¤åˆ†æçµæœ',
        'data/final_sybil_scores.csv': 'æœ€çµ‚é¢¨éšªè©•åˆ†',
        'data/cluster_summary.csv': 'é›†ç¾¤æ‘˜è¦'
    }
    
    existing_files = {}
    missing_files = []
    
    for file_path, description in required_files.items():
        if os.path.exists(file_path):
            try:
                if file_path.endswith('.csv'):
                    df = pd.read_csv(file_path)
                    rows = len(df)
                    cols = len(df.columns)
                    print(f"âœ… {file_path}: {description} ({rows} è¡Œ, {cols} åˆ—)")
                    existing_files[file_path] = {'rows': rows, 'cols': cols, 'type': 'csv'}
                elif file_path.endswith('.json'):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    print(f"âœ… {file_path}: {description} (JSON)")
                    existing_files[file_path] = {'type': 'json', 'size': len(str(data))}
            except Exception as e:
                print(f"âš ï¸  {file_path}: å­˜åœ¨ä½†ç„¡æ³•è®€å– - {str(e)}")
        else:
            print(f"âŒ {file_path}: {description} (ç¼ºå¤±)")
            missing_files.append(file_path)
    
    # æª¢æŸ¥è…³æœ¬æ–‡ä»¶
    print("\nğŸ Python è…³æœ¬:")
    script_files = [
        'scripts/fetch_tx_history.py',
        'scripts/cluster_analysis.py', 
        'scripts/sybil_score.py',
        'streamlit_sybil_dashboard.py'
    ]
    
    for script in script_files:
        if os.path.exists(script):
            print(f"âœ… {script}")
        else:
            print(f"âŒ {script}")
    
    # åˆ†æç•¶å‰ç‹€æ…‹
    print("\nğŸ“‹ ç‹€æ…‹åˆ†æ:")
    if 'data/sybil_addresses.csv' in existing_files:
        print("âœ… å·²æœ‰åŸå§‹åœ°å€æ•¸æ“š")
    else:
        print("âŒ ç¼ºå°‘åŸå§‹åœ°å€æ•¸æ“š")
    
    if 'data/layerzero_transactions.csv' in existing_files:
        print("âœ… å·²æœ‰äº¤æ˜“æ•¸æ“š")
    else:
        print("âŒ ç¼ºå°‘äº¤æ˜“æ•¸æ“š - éœ€è¦é‹è¡Œ fetch_tx_history.py")
    
    if 'data/cluster_analysis_report.json' in existing_files:
        print("âœ… å·²æœ‰é›†ç¾¤åˆ†æ")
    else:
        print("âŒ ç¼ºå°‘é›†ç¾¤åˆ†æ - éœ€è¦é‹è¡Œ cluster_analysis.py")
    
    if 'data/final_sybil_scores.csv' in existing_files:
        print("âœ… å·²æœ‰é¢¨éšªè©•åˆ†")
    else:
        print("âŒ ç¼ºå°‘é¢¨éšªè©•åˆ† - éœ€è¦é‹è¡Œ sybil_score.py")
    
    # æä¾›ä¿®å¾©å»ºè­°
    print("\nğŸ’¡ ä¿®å¾©å»ºè­°:")
    if missing_files:
        print("ğŸ”„ æŒ‰é †åºåŸ·è¡Œä»¥ä¸‹å‘½ä»¤ä¾†ç”Ÿæˆç¼ºå¤±çš„æ–‡ä»¶:")
        
        if 'data/layerzero_transactions.csv' in missing_files:
            print("1. python scripts/fetch_tx_history.py")
        
        if 'data/cluster_analysis_report.json' in missing_files:
            print("2. python scripts/cluster_analysis.py")
        
        if 'data/final_sybil_scores.csv' in missing_files:
            print("3. python scripts/sybil_score.py")
        
        print("4. streamlit run streamlit_sybil_dashboard.py")
    else:
        print("ğŸ‰ æ‰€æœ‰æ–‡ä»¶éƒ½å­˜åœ¨ï¼Œå¯ä»¥ç›´æ¥é‹è¡Œ Streamlit dashboard!")
        print("   streamlit run streamlit_sybil_dashboard.py")
    
    return existing_files, missing_files

def create_sample_data():
    """å‰µå»ºç¤ºä¾‹æ•¸æ“šç”¨æ–¼æ¼”ç¤º"""
    print("\nğŸ­ å‰µå»ºç¤ºä¾‹æ•¸æ“šç”¨æ–¼æ¼”ç¤º...")
    
    # ç¢ºä¿ data ç›®éŒ„å­˜åœ¨
    os.makedirs('data', exist_ok=True)
    
    # å¦‚æœæ²’æœ‰äº¤æ˜“æ•¸æ“šï¼Œå‰µå»ºç¤ºä¾‹æ•¸æ“š
    if not os.path.exists('data/layerzero_transactions.csv'):
        print("ğŸ“ å‰µå»ºç¤ºä¾‹äº¤æ˜“æ•¸æ“š...")
        
        # åŸºæ–¼ä½ ä¹‹å‰çš„çœŸå¯¦æ•¸æ“šå‰µå»ºç¤ºä¾‹
        sample_data = []
        addresses = [f"0x{i:040x}" for i in range(20)]  # 20å€‹ç¤ºä¾‹åœ°å€
        
        import datetime
        base_time = datetime.datetime(2023, 7, 13, 20, 52, 13)
        
        for i, addr in enumerate(addresses):
            for j in range(8):  # æ¯å€‹åœ°å€8ç­†äº¤æ˜“
                sample_data.append({
                    'address': addr,
                    'guid': f'guid_{i}_{j}',
                    'src_tx_hash': f'0x{(i*8+j):064x}',
                    'dst_tx_hash': f'0x{(i*8+j+1000):064x}',
                    'src_eid': [102, 110, 106, 102, 109, 106, 110, 111][j],
                    'dst_eid': [116, 106, 102, 109, 106, 110, 111, 106][j],
                    'src_chain': 'EID_' + str([102, 110, 106, 102, 109, 106, 110, 111][j]),
                    'dst_chain': 'EID_' + str([116, 106, 102, 109, 106, 110, 111, 106][j]),
                    'nonce': j + 1,
                    'status': 'DELIVERED',
                    'source_status': 'DELIVERED',
                    'dest_status': 'DELIVERED',
                    'block_timestamp': base_time + datetime.timedelta(minutes=i*2+j*5),
                    'pathway_id': f'pathway_{j}'
                })
        
        df = pd.DataFrame(sample_data)
        df.to_csv('data/layerzero_transactions.csv', index=False)
        print("âœ… ç¤ºä¾‹äº¤æ˜“æ•¸æ“šå·²å‰µå»º")
    
    # å‰µå»ºç¤ºä¾‹é¢¨éšªè©•åˆ†æ•¸æ“š
    if not os.path.exists('data/final_sybil_scores.csv'):
        print("ğŸ“ å‰µå»ºç¤ºä¾‹é¢¨éšªè©•åˆ†æ•¸æ“š...")
        
        addresses = [f"0x{i:040x}" for i in range(20)]
        scores_data = []
        
        for i, addr in enumerate(addresses):
            scores_data.append({
                'address': addr,
                'composite_score': 85 + (i % 5),  # 85-89åˆ†
                'risk_level': 'HIGH - é«˜å¥³å·«é¢¨éšª',
                'confidence': 'VERY_HIGH',
                'pathway_score': 100,
                'temporal_score': 85 + (i % 10),
                'behavioral_score': 80 + (i % 15),
                'network_score': 85 + (i % 8)
            })
        
        df_scores = pd.DataFrame(scores_data)
        df_scores.to_csv('data/final_sybil_scores.csv', index=False)
        print("âœ… ç¤ºä¾‹é¢¨éšªè©•åˆ†æ•¸æ“šå·²å‰µå»º")
    
    # å‰µå»ºç¤ºä¾‹é›†ç¾¤åˆ†ææ•¸æ“š
    if not os.path.exists('data/cluster_analysis_report.json'):
        print("ğŸ“ å‰µå»ºç¤ºä¾‹é›†ç¾¤åˆ†ææ•¸æ“š...")
        
        cluster_report = {
            "analysis_timestamp": "2025-09-11T11:46:39",
            "total_addresses": 20,
            "total_transactions": 160,
            "clusters": {
                "temporal": {
                    "count": 3,
                    "details": [
                        {
                            "cluster_id": 0,
                            "addresses": [f"0x{i:040x}" for i in range(20)],
                            "size": 20,
                            "transaction_count": 160
                        }
                    ]
                },
                "pathway": {
                    "count": 1,
                    "details": [
                        {
                            "cluster_id": 0,
                            "addresses": [f"0x{i:040x}" for i in range(20)],
                            "size": 20,
                            "pattern": "102->116|110->106|106->102|102->109|109->106|106->110|110->111|111->106"
                        }
                    ]
                },
                "similarity": {
                    "count": 3,
                    "details": [
                        {
                            "cluster_id": 0,
                            "addresses": [f"0x{i:040x}" for i in range(12)],
                            "size": 12
                        },
                        {
                            "cluster_id": 1,
                            "addresses": [f"0x{i:040x}" for i in range(12, 15)],
                            "size": 3
                        },
                        {
                            "cluster_id": 2,
                            "addresses": [f"0x{i:040x}" for i in range(15, 19)],
                            "size": 4
                        }
                    ]
                }
            }
        }
        
        with open('data/cluster_analysis_report.json', 'w', encoding='utf-8') as f:
            json.dump(cluster_report, f, indent=2, ensure_ascii=False)
        print("âœ… ç¤ºä¾‹é›†ç¾¤åˆ†ææ•¸æ“šå·²å‰µå»º")

if __name__ == "__main__":
    existing, missing = check_project_status()
    
    if missing:
        print(f"\nâš ï¸  ç™¼ç¾ {len(missing)} å€‹ç¼ºå¤±æ–‡ä»¶")
        
        response = input("\nâ“ æ˜¯å¦å‰µå»ºç¤ºä¾‹æ•¸æ“šç”¨æ–¼æ¼”ç¤º? (y/n): ")
        if response.lower() == 'y':
            create_sample_data()
            print("\nğŸ‰ ç¤ºä¾‹æ•¸æ“šå‰µå»ºå®Œæˆï¼ç¾åœ¨å¯ä»¥é‹è¡Œ Streamlit dashboard äº†:")
            print("   streamlit run streamlit_sybil_dashboard.py")
        else:
            print("\nğŸ’¡ è«‹æŒ‰ç…§ä¿®å¾©å»ºè­°é †åºåŸ·è¡Œè…³æœ¬ä¾†ç”ŸæˆçœŸå¯¦æ•¸æ“š")
    else:
        print("\nğŸš€ ä¸€åˆ‡å°±ç·’ï¼é‹è¡Œä»¥ä¸‹å‘½ä»¤å•Ÿå‹• dashboard:")
        print("   streamlit run streamlit_sybil_dashboard.py")